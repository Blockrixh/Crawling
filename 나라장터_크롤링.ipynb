{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "049318c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#내가 항상 불러오는 것들. 그냥 copy paste 함.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#브라우저 꺼짐 방지\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option('detach', True)\n",
    "\n",
    "#기본 비어있는 리스트를 만듬. 여기에 값들을 저장할 예정\n",
    "li_task = []\n",
    "li_title = []\n",
    "li_notice = []\n",
    "li_demand = []\n",
    "\n",
    "#총 10페이지까지(1페이지에 10개, 10*10 100개를 크롤링할 예정)할 예정이라 for 문 돌림\n",
    "#이때 url을 살펴보면 페이지 넘버에 따라 뒤 숫자가 바뀌는 유형이었음. 만약에 페이지 넘버가 안바뀐다면? selenium의 click매소드를  활용해서 계속 페이지를 넘겨줬을 예정\n",
    "for i in range(1, 11):\n",
    "    url = f'https://www.g2b.go.kr:8101/ep/tbid/tbidList.do?area=&bidNm=%B0%F8%B0%A3%C1%A4%BA%B8&bidSearchType=1&fromBidDt=2023%2F01%2F07&fromOpenBidDt=&instNm=&maxPageViewNoByWshan=2&radOrgan=1&regYn=Y&searchDtType=1&searchType=1&taskClCds=&toBidDt=2023%2F07%2F10&toOpenBidDt=&currentPageNo={i}'\n",
    "\n",
    "    service = Service('chromedriver.exe') #크롬드라이버 정의\n",
    "    browser = webdriver.Chrome(service=service, options = chrome_options) #크롬드라이버 안꺼지게하기\n",
    "    browser.implicitly_wait(10) #대기. 대기 안하면 페이지 못불러오는 경우가 있어서.. 2~10 넉넉히. 5도 괜찮음\n",
    "    browser.maximize_window() #창 최대화. 꼭 필요하지는 않음.\n",
    "    browser.get(url) #드라이버 실행\n",
    "\n",
    "\n",
    "\n",
    "    # 새로운 검색창에서 소스 받아오기. 매 페이지가 바뀔때마다 내용이 바뀌기 때문에 매번 소스를 받아와야 해서 for 문안에 넣음\n",
    "\n",
    "    html = browser.page_source  # 소스 받아오고\n",
    "    soup = BeautifulSoup(html, 'html.parser')  # soup만들기\n",
    "    contents = soup.find_all('tbody') #tbody에 id나 class 정의가 없었음. 딱 1개여서.\n",
    "    infos = contents[0].find_all('tr') #모든 데이터들이 table에 row로 구성되어 있었음(10개). 그래서 모든 tr을 infos에 넣음\\\n",
    "\n",
    "    for info in infos: #10개의 infos를 하나씩 분리하는 과정. 내가 필요한 것만 뽑았음. 다른것도 뽑을 수 있음\n",
    "        task = info.select_one('td:nth-of-type(1)').text #업무 추출\n",
    "        title = info.select_one('td:nth-of-type(4)').text #공고명\n",
    "        notice = info.select_one('td:nth-of-type(5)').text #공고기관\n",
    "        demand = info.select_one('td:nth-of-type(6)').text #수요기관\n",
    "        li_task.append(task) #빈 리스트에 업무 하나씩 넣기\n",
    "        li_title.append(title) #빈 리스트에 공고명 하나씩 넣기\n",
    "        li_notice.append(notice)#빈 리스트에 공고기관 하나씩 넣기\n",
    "        li_demand.append(demand) #빈 리스트에 수요기관 하나씩 넣기\n",
    "    \n",
    "    browser.quit() #브라우저 종료. 왜냐면 두번째 페이지를 새롭게 열거라서. 이걸 안하면 크롬이 10번 반복되는 동안 총 10개나 생성됨\n",
    "\n",
    "\n",
    "    \n",
    "#끝났으니까 csv로 추출하기.\n",
    "df = pd.DataFrame({\n",
    "    '업무': li_task,\n",
    "    '공고명': li_title,\n",
    "    '공고기관': li_notice,\n",
    "    '수요기관':  li_demand\n",
    "})\n",
    "\n",
    "df.to_csv('나라장터.csv', encoding=\"utf-8-sig\",index = False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5816a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
